{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c31ef9e",
   "metadata": {},
   "source": [
    "# Introduction to Big Data\n",
    "\n",
    "## Semester End Exam\n",
    "\n",
    "####  Date   : March 2022\n",
    "#### Duration : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a13c5",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS:\n",
    "\n",
    "1.\tCandidates should answer all the questions in the same order provided in the question paper.\n",
    "2.\tAny activity that compromises the integrity of the examination will not be permitted.\n",
    "3.\tStudents should complete the examination within the provided timeline.\n",
    "4.  Once completed use <b> File> Dowload as > HTML(.html) </b> to download the notebook as html page into local drive. you must specifying path and check that its downloaded propery and shows output of each cell without error .\n",
    "4.\tCandidates are expected to check and ensure that the correct answer file (.html format) is uploaded in LMS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14838ec",
   "metadata": {},
   "source": [
    "### DATA DESCRIPTION: \n",
    "\n",
    "\n",
    "This dataset is real-state data of bangalore. This file has 9 attributes. Our main objective is to  process the data set using spark libraries to check the programing & conceptural understanding .\n",
    "\n",
    "### ATTRIBUTES:\n",
    "\n",
    "Dataset contains a total of 9 attributes as shown below \n",
    "\n",
    "- area_type :  Type of measurment area\n",
    "\n",
    "- availability: Availabily status/date \n",
    "\n",
    "- location: locality of bengaluru \n",
    "\n",
    "- size: Number of bedrooms/ bhk\n",
    "\n",
    "- society: Name of Socity\n",
    "\n",
    "- total_sqft:  house Area  in sq-feet  \n",
    "\n",
    "- bath: Number of bathrooms  \n",
    "\n",
    "- balcony: Number of balaconies\n",
    "\n",
    "- price: Price in lakh \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66694936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count, col,avg\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import FMRegressor\n",
    "from pyspark.ml import Pipeline\n",
    " \n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b58a93f-9f7c-43b3-b7b3-9384bc012372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c944534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3df16af",
   "metadata": {},
   "source": [
    "## Section C\n",
    "\n",
    "Bangalore Housing Dataset is provided and loaded as Spark-DataFrame.  Using Spark libraries execute the steps, as questioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbf5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update this path for docker image\n",
    "file_path='Bengaluru_House_Data.csv' \n",
    "\n",
    "#file_path='Bengaluru_House_Data.csv' \n",
    "\n",
    "#load the data spark Dataframe \n",
    "#df=spark.read.format(\"csv\").option(\"header\", \"True\").option(\"inferschema\",\"True\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a545e6a-031b-417e-bbd4-e9345b665cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferschema\",\"True\").load(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "927ea3d0",
   "metadata": {},
   "source": [
    "### 3.a.  Using PySpark and Spark-SQL libraries process the dataset to find out solutions of queries mentioned below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95b808bf",
   "metadata": {},
   "source": [
    "### 3.a.(i).\tHow many total numbers of housing-properties are listed in the dataset? (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0065d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "# Ans : 13320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68943766-387d-4817-be1f-a091ec43f142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13320"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16a90663",
   "metadata": {},
   "source": [
    "### 3.a.(ii). What is the average price of ‘3 BHK’ size housing-properties in ‘Electronic City’ location?( 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d7702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter( (df.location == 'Electronic City') & (df.size == '3 BHK') ).count()\n",
    "# Ans:76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41794a92-08dd-4d13-9ff7-1a6d245b0788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        avg_price|\n",
      "+-----------------+\n",
      "|67.22092105263157|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.location=='Electronic City')&(df.size=='3 BHK')).select(avg('price').alias('avg_price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f3870e5-879e-4fd8-b57f-cc52c6908b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "|Min_Price|Max_Price|         Avg_Price|\n",
      "+---------+---------+------------------+\n",
      "|      8.0|   3600.0|112.56562650150138|\n",
      "+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    min(\"price\").alias(\"Min_Price\"),\n",
    "    max(\"price\").alias(\"Max_Price\"),\n",
    "    avg(\"price\").alias(\"Avg_Price\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb913df",
   "metadata": {},
   "source": [
    "### 3.a.(iii).   Which locality has maximum availability of  ‘Ready To Move’ housing-properties?  ( 5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012f2158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  location|count|\n",
      "+----------+-----+\n",
      "|Whitefield|  331|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.availability == 'Ready To Move').groupBy('location').count().orderBy(['count'],ascending = [False]).show(1)\n",
    "# Ans: Whitefield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deebf590-2840-45fd-81fb-815e24750a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  location|count|\n",
      "+----------+-----+\n",
      "|Whitefield|  331|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.availability=='Ready To Move').groupby(\"location\").count().orderBy('count', ascending=[False]).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775c6d3-d512-414f-9be1-c925e79dad00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f9efc-3e01-4a46-af3b-6a4b2350507c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e32771e7",
   "metadata": {},
   "source": [
    "### 3.b. Using Spark ML libraries process the Dataframe as questioned below (in order to build a PySpark  regression ML-model on provided Bangalore Housing data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0db7b62",
   "metadata": {},
   "source": [
    "#### 3.b.(i).  i.\tFor features, having more than one third of their entries as missing/null, replace null cells with the new value as ‘missing’.  For the remaining missing/null values - remove the corresponding row entry from the DataFrame. (3 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79aef042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "|area_type|availability|location|size|society|total_sqft|bath|balcony|price|\n",
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "|        0|           0|       1|  16|   5502|         0|  73|    609|    0|\n",
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch column wise mssing count\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ba4e6a2-c951-4d27-b03e-0bf532dca74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "|area_type|availability|location|size|society|total_sqft|bath|balcony|price|\n",
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "|        0|           0|       1|  16|   5502|         0|  73|    609|    0|\n",
      "+---------+------------+--------+----+-------+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a73420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repalce society column null entries with a new value as 'missing'\n",
    "df = df.na.fill(\"missing\",'society');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5550d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the corresponding row entry with rest of the null entries\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a923974",
   "metadata": {},
   "source": [
    "### 3.b.(ii). Convert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore.(3 marks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c267a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "indexers = [StringIndexer(inputCol=column,\n",
    "                          outputCol=column+\"_indexex\").fit(df) for column in ['area_type',\n",
    "                                                                            'availability','location',\n",
    "                                                                            'size','society','total_sqft']]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923edbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternate code for doing StringIndexer transformation\n",
    "# indexer_model = StringIndexer(inputCols = [\"area_type\", \"availability\", \"location\", \"size\", \"total_sqft\"],                         \n",
    "#                          outputCols =[\"area_type_indexed\", \"availability_indexed\",\n",
    "#                                       \"location_indexed\",\"size_indexed\",\"total_sqft_indexed\"]).fit(df)\n",
    "# df_indexed = indexer_model.transform(df)\n",
    "# df_indexed = df_indexed.drop(\"area_type\", \"availability\", \"location\",'size',\"total_sqft\")\n",
    "# df_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5dd84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the string columns \n",
    "df = df.drop('area_type','availability','location', 'size','society','total_sqft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef0d70",
   "metadata": {},
   "source": [
    "### 3.b.(iii). Using vectorAssembler combines all columns (except target column i.e. 'price') of spark DataFrame into single column (named as features). Make sure DataFrame now contains only two columns features and price.(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01909e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price',\n",
       " 'area_type_indexex',\n",
       " 'availability_indexex',\n",
       " 'location_indexex',\n",
       " 'size_indexex',\n",
       " 'society_indexex',\n",
       " 'total_sqft_indexex',\n",
       " 'features']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_col = df.columns\n",
    "features_col.remove('price')\n",
    "\n",
    "# Create the VectorAssembler object\n",
    "assembler = VectorAssembler(inputCols= features_col, outputCol= \"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "\n",
    "df= df_assembled.drop('bath',\n",
    " 'balcony',\n",
    " 'location_index',\n",
    " 'size_index',\n",
    " 'total_sqft_index',\n",
    " 'society_index','availability_index',\n",
    " 'area_type_index')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb6ab8e0",
   "metadata": {},
   "source": [
    "### 3.b.(iv) Split the vectorized dataframe into training and test sets with one fourth records being held for testing. (1 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33086fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split \n",
    "df_train , df_test = df.randomSplit([0.75,0.25], seed = 2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23115a94",
   "metadata": {},
   "source": [
    "### 3.b.(v).\tTrain default LinearRegression model with features as 'featuresCol'   and ‘price’ as label and measure RMSE value on test set (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bf27e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 120.219\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(df_train)\n",
    "lr_predicton  = lr_model.transform(df_test)\n",
    "#df_predicton.show()\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(lr_predicton)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c58068",
   "metadata": {},
   "source": [
    "### 3.b.(vi).\tTrain default Factorization machines regressor model with features as 'featuresCol'   and ‘price’ as label and measure RMSE value on test set.  Which among LinearRegression and Factorization machines regressor model performs better? (marks 4 +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad8539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 9439.28\n"
     ]
    }
   ],
   "source": [
    "#Factorization machines regressor\n",
    "from pyspark.ml.regression import FMRegressor\n",
    "fm =  FMRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
    "fm_model = fm.fit(df_train)\n",
    "fm_predicton  = fm_model.transform(df_test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(fm_predicton)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0660c",
   "metadata": {},
   "source": [
    "Clearly LinearRegression peforms better than FMRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
